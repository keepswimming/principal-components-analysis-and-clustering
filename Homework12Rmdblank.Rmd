---
title: "Homework 12 R markdown"
author: "Rita Miller"
date: '`r Sys.Date()`'
output:
  html_document:
    fig_height: 4
    fig_width: 4.5
  word_document:
    fig_height: 4
    fig_width: 4.5
  pdf_document:
    fig_height: 4
    fig_width: 4.5
---


```{r, setup, include=FALSE}
require(mosaic)   # Load additional packages here 

# Some customization.  You can alter or delete as desired (if you know what you are doing).
#trellis.par.set(theme=theme.mosaic()) # change default color scheme for lattice
knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```

#### **Intellectual Property:**
These problems are the intellectual property of the instructors and may not be reproduced outside of this course.

**.Rmd output format**:  The setting in this markdown document is `word_document`, since You also have options for `word_document` or `pdf_document`.

###################################
## Problem 1: Clustering Methods ##
###################################

In this problem, you will explore clustering methods.

**Data Set**: Load the *wine.csv* data set (from the **rattle** package).

Description from the documentation for the R package **rattle**: “The wine dataset contains the results of a chemical analysis of wines grown in a specific area of Italy. Three types of wine are represented in the 178 samples, with the results of 13 chemical analyses recorded for each sample.” That is, we have n = 178 wine samples, each of which has p = 13 measured characteristics.
 
```{r,echo=FALSE}
wine <- read.csv("wine.csv")
x = wine
#View(wine)
#names(wine)
#dim(wine)
```
```{r}
# Load packages here 
library(rattle)
library(dplyr)
# For visuals
library(ggformula)
library(ggdendro)
library(gridExtra)
library(ggplot2)
```

### **Important Note**: 
It is carefully noted in each problem to standardize the data.  Attention to those instructions will help you obtain the correct answers.

After loading in the data from the *wine.csv* file, store the 13 numeric variables in a data frame **x**.

### Question 1 **(2 points)**: 

Compute the means and standard deviations for all the variables.  Compare the means and standard deviations between the thirteen variables, using these values to explain why it is a good idea to standardize the variables before clustering.  Include at least one numeric computation to support your explanation.

### Standardize the numeric variables in **x** 
and store the results in **x.scale**.
```{r}
# measurements only
n = dim(x)[1]
#p = dim(x)[2]
#pselect = dim(xselect)[2]

#mean and sd
apply(x,2,mean)
apply(x,2,sd)

# scaling visibly required to allow all predictors to contribute to similarity measure
x.scale = scale(x)
```
#**Text Answer**: 
The concept of standardization/normalization comes into picture when continuous independent variables are measured at different scales. It means these variables do not give equal contribution to the analysis. It is vital to standardize variables before running Cluster Analysis (CA), because CA techniques depend on the concept of measuring the distance between the different observations we're trying to cluster. For instance, Alcohol seems to be in the 10's scale, while Proline appears to be in the  1000's scale. When a variable is measured on a different scale than other variables, then whatever measure we use may be overly influenced by that variable.
```
***
### Hierarchical Clustering

### Question 2 **(2 points)**:

Using Euclidean distance with **x.scale**, fit the hierarchical model using complete linkage.  

Produce a dendrogram of all the clusters ; use the "Embed Image" button to embed the plot in the Canvas question.

**Graph Answer** 
```{r,echo=FALSE, fig.width=6}
# # Euclidean distance
dist.x.scale = dist(x.scale, method = "euclidean")

#fit the hierarchical model using complete linkage.  
hc.fit = hclust(dist.x.scale, method = "complete")  
linktype = "Complete Linkage"

#plot(hc.fit) #plots in BW
```
```{r}
############# complete  linkage ############
# using selected p = 3 measurements
hc.fit = hclust(dist.x.scale, method = "complete")  # Euclidean
linktype = "Complete Linkage"

# distance at which merge via complete linkage occurs
hc.fit$height
hc.4321 = hc.fit$height[(n-4):(n-1)]
hc.avg = (hc.fit$height[(n-3):(n-1)]+hc.fit$height[(n-4):(n-2)])/2

# obtaining cluster labels
hc.fit$height[(n-4):(n-1)]
nclust=3   #--------------------------> changed to 3
htclust = 7.56 # mean(hc.fit$height[(n-2):(n-1)])   
membclust = cutree(hc.fit,k=nclust) # cutree(hc.fit,h = htclust)
```

```{r}
#############  visualize in dendrogram ############# 
dend.form = as.dendrogram(hc.fit)
dend.merge <- ggdendrogram(dend.form, rotate = F,labels=F) + 
  labs(title = linktype) +
  geom_hline(yintercept=hc.4321, linetype="dashed", 
             color = c("red","blue","gold3","gray"))  
dend.merge
dend.merge +
  geom_hline(yintercept=hc.avg, size = 2,
             color = c(rgb(.5,0,1,0.5),rgb(.5,1,0,0.5),rgb(.5,.5,.2,0.5))) 

```
### Question 3 **(2 points)**K: ~9.65 

List an appropriate “height” (corresponding to the value of the distance measure) on the dendrogram for complete linkage that would produce three clusters.

(The autograder will accept all answers within an appropriate range of values  - respond to *two* decimal places.)

**Numeric Answer (AUTOGRADED)**  


### Question 4 **(1 point)**: K

Using Euclidean distance with **x.scale**, fit the hierarchical model using each of single linkage and average linkage, as well as complete linkage. Which of the linkage methods appears to produce clusters that are most similarly-sized as they merge?
```{r}
#single linkage
hc.fit.single = hclust(dist.x.scale, method = "single")

linktype = "Single Linkage"

plot(hc.fit.single) #Very R-skewed
```
```{r}
#average linkage
hc.fit.avg = hclust(dist.x.scale, method = "average")

linktype = "Average Linkage"

plot(hc.fit.avg)#slightly R-skewed
```

**Multiple choice Answer (AUTOGRADED)**:  one of  
*Complete linkage,   <------
Simple linkage,  
Average linkage  


### Question 5 **(1 point)**: K

Suppose we had further information that there are three types of wine, approximately equally represented, included in this data set.  Which visually appears to be the most reasonable linkage method to designate those three clusters?

**Multiple choice Answer (AUTOGRADED)**:  one of  
*Complete linkage, <----- 
Simple linkage,  
Average linkage  


### Question 6 **(2 points)**: 

Explain what you see visually in the dendrograms for the three linkage methods that supports your answer in the previous question.

**Multiple dropdowns**:  At the (length, *height, wine samples) at which the data group into three clusters (corresponding to the distance between clusters), the selected linkage method produces clusters that have (very different, *very similar) numbers of wine samples in each of the three clusters.  

The other two unselected methods do not produce equally sized clusters, as illustrated by the (*one, two)  cluster(s) having many more wine samples than (*the other one cluster, the other two cluster) (with very few wine samples).

### Question 7 **(2 points)**:

Using the linkage method you selected to best designate three types of wine, for the split of the data in three clusters, make a plot of *Alcohol* versus *Dilution* marked by the clusters (using three different colors and/or symbols, along with a legend).

Use the "Embed Image" button  to upload your plot in Canvas question.

**Graph Answer**  

```{r,echo=FALSE}
#Complete linkage
#make a plot of *Alcohol* versus *Dilution* marked by the clusters (using three different colors and/or symbols, along with a legend).

############# complete  linkage ############
# using selected p = 3 measurements
hc.fit = hclust(dist.x.scale, method = "complete")  # Euclidean
linktype = "Complete Linkage"

# distance at which merge via complete linkage occurs
hc.fit$height
hc.4321 = hc.fit$height[(n-4):(n-1)]
hc.avg = (hc.fit$height[(n-3):(n-1)]+hc.fit$height[(n-4):(n-2)])/2

# obtaining cluster labels
hc.fit$height[(n-4):(n-1)]
nclust=3   
htclust = 7.56  # mean(hc.fit$height[(n-2):(n-1)])   
membclust = cutree(hc.fit,k=nclust) # cutree(hc.fit,h = htclust) 

#plot the observations
#setting up colors to use
colused = c("green", "blue", "purple")
plot(x$Alcohol, x$Dilution, pch=16, xlab = "Alcohol", ylab = "Dilution", main=paste(nclust, "Clusters Joined by Complete Linkage"))

#color the observations based on their class. 
for (i in 1:2){
  points(x$Alcohol[membclust == i],x$Dilution[membclust == i],pch=16,col=colused[i])
} 

```

### Nonhierarchical Clustering

Now we consider using nonhierarchical (*K*-means) clustering to split the data into clusters.

### Question 8 **(2 points)**:

For *K*-means clustering, use multiple initial random splits to produce *K* = 5, 4, 3, and 2 clusters.  Use tables or plots to investigate the clustering memberships across various initial splits, for each value of *K*.  Which number(s) of clusters seem to produce very consistent cluster memberships (matching more than 95% of memberships between nearly all initial splits) across different initial splits?  Select all *K* that apply.

**Important**:  compare two memberships for multiple different initial splits.


```{r}
par(mfrow=c(2,2))
colused = c("turquoise3", "red", "black", "orange","blue", "slateblue",  "purple","green", "violetred" )
#repeat the following a few times
nclust=5
memb = kmeans(x.scale,nclust)$cluster
plot(x$Alcohol, x$Dilution,pch=16,main=paste(nclust," clusters determined by K-means"))
for (i in 1:9)  points(x$Alcohol[memb == i],x$Dilution[memb == i],pch=16,col=colused[i])
nclust=4
memb = kmeans(x.scale,nclust)$cluster
plot(x$Alcohol, x$Dilution,pch=16,main=paste(nclust," clusters determined by K-means"))
for (i in 1:9)  points(x$Alcohol[memb == i],x$Dilution[memb == i],pch=16,col=colused[i])
nclust=3
memb = kmeans(x.scale,nclust)$cluster
plot(x$Alcohol, x$Dilution,pch=16,main=paste(nclust," clusters determined by K-means"))
for (i in 1:9)  points(x$Alcohol[memb == i],x$Dilution[memb == i],pch=16,col=colused[i])
nclust=2
memb = kmeans(x.scale,nclust)$cluster
plot(x$Alcohol, x$Dilution,pch=16,main=paste(nclust," clusters determined by K-means"))
for (i in 1:1)  points(x$Alcohol[memb == i],x$Dilution[memb == i],pch=16,col=colused[i])
```


**Multiple select (AUTOGRADED)**:  any of  
2,  
3,  #<-------
4, and/or  
5

### Final Nonhierarchical Clustering

Starting with ``set.seed(12)`` to set the initial split, use nonhierarchical (*K*-means) clustering to determine cluster membership for three clusters (corresponding to the three types of wine).  
How many wine samples are in each cluster?

```{r,echo=FALSE}
RNGkind(sample.kind = "Rejection") #random number generation or knit doc
#set seed
set.seed(12)
#set number of clusters
nclust=3
#perform kmeans on the scaled data and store the cluster assignment into a vector
nhc.memb = kmeans(x.scale, nclust)$cluster

#count each cluster membership to get totals.
summary(as.factor(nhc.memb))

#table(nhc.memb)
```
1  2  3 
65 62 51
### Question 9 **(1 point)**:

Wine samples in Cluster 1: 65  ?*62

**Numeric Answer (AUTOGRADED)**  

### Question 10 **(1 point)**: 62, 65, 51 is wrong*, *****use 65, 62, 51*****

Wine samples in Cluster 2: 65

**Numeric Answer (AUTOGRADED)**  

### Question 11 **(1 point)**:

Wine samples in Cluster 3: 51

**Numeric Answer (AUTOGRADED)**  


### Question 12 **(2 points)**:

For splitting into three clusters, compare the cluster membership of hierarchical clustering (using the linkage method you selected when creating three clusters to designate three types of wine) to the cluster membership of K-means clustering (using the cluster membership from the previous question).  What proportion of the cluster memberships match between the hierarchical and nonhierarchical clustering methods?

Proportion that match $\approx$ 

(respond to *two* decimal places)

**Important**:  cluster *labels* do NOT necessarily match.

```{r,echo=FALSE}
#compare hierarchy and non-hierarchy clustering results, and see where they both agreed/disagreed. 
prop.table = table(membclust, nhc.memb)
#calculate the proportion of observations they "agreed" upon. 
sum(diag(prop.table))/sum(prop.table) # 0.42 
```
------>0.84 

**Numeric Answer (AUTOGRADED)**  

############################
## Problem 2: PCA methods ##
############################

We will continue to use the wine data set from Problem 1.  We have *n* = 178 wine samples, each of which has *p* = 13 measured characteristics.

Load in the data from the **wine.csv** file.  Store the 13 numeric variables in a data frame **x**.

We wish to use PCA to identify which variables are most meaningful for describing this dataset.  Use the ``prcomp`` function, with ``scale=T``, to find the principal components. 

```{r}
wine <- read.csv("wine.csv")
#Store the 13 numeric variables in a data frame **x**.
x = wine
```
```{r}
#Fitting principal components
#fitting to full set of --measurements
# include scaling due to widely varied magnitudes
pc.info = prcomp(x, center = T, scale = T) #center and scale

```

### Question 13 **(2 points)**K: -0.144 

Look at the loadings for the first principal component.  What is the loading for the variable *Alcohol*?

(respond to *three* decimal places)

```{r}
# loadings of principal components
pc.info$rotation ##check the coefficient for alcohol in PC1.
```

**Numeric Answer (AUTOGRADED)**  


### Question 14 **(1 point)**: K

Which variable appears to contribute the **least** to the first principal component?

**Multiple choice (AUTOGRADED)**:  one of  
Alcohol  
Malic  
Ash <------------------ 
Alcalinity  
Magnesium  
Phenols  
Flavanoids  
Nonflavanoids  
Proanthocyanins  
Color  
Hue  
Dilution  
Proline 


### Question 15 **(1 point)**: 0.362

What is the PVE for the first principal component?

(enter as a proportion, number between 0 to 1, and report to *three* decimal place)

```{r,echo=FALSE}
summary(pc.info) ##check proportion of variance explained (PVE) for PC1: 
```

**Numeric Answer (AUTOGRADED)**  


### Question 16 **(2 points)**k: 5  (based on the scree plot)

How many principal components would need to be used to explain about 80% of the variability in the data?

```{r}
#testing
#plot(CumulativePVE, type = "o", ylab="Cumulative PVE", xlab="Principal Component")
```

**Numeric (integer) Answer (AUTOGRADED)**  

### Scores

On a biplot of the data, wine sample #159 appears to be an outlier in the space of principal components 1 and 2.  What are the principal component 1 and 2 score values (that is, the coordinates in the space of principal components 1 and 2) for wine sample #159?
```{r,echo=FALSE}
# plotting score vectors + loadings of first two principal components
#biplot(pc.info, choices = 1:2, scale = 0) 

```
```{r,echo=FALSE}
#observation 159 seems to be outlier, get its coordinates. 
pc.info$x[159,1:2]
```
 PC1      PC2 
1.045233 3.505202 

### Question 17 **(1 point)**k: # 1.045 

Principal component 1 score value $\approx$

(report to *three* decimal places)

**Numeric Answer (AUTOGRADED)**  


### Question 18 **(1 point)**: 3.505 

Principal component 2 score value $\approx$

(report to *three* decimal places)

**Numeric Answer (AUTOGRADED)**  

############################################
## Problem 3: Gene Expression Application ##
############################################

Find the gene expression data set **GeneExpression.csv** in the online course.  There are 40 tissue samples, each with measurements on 1000 genes.  Note that this dataset is “transposed” from typical format; that is, the variables (gene expression measurements) are listed in the rows, and the data points (tissue samples) that we want to group or identify are listed in the columns.  That is, we have n = 40 tissue samples, each of which has p = 1000 observed gene expression measurements.

The goal is to distinguish between healthy and diseased tissue samples.

Data preparation:

   1.  Load in the data using ``read.csv())``. Note the ``header=F`` argument is used to identify that there are no column names.  
   2.  Be sure to transpose the data frame before using it for analysis, using the function ``t()``.  
   3. Standardize the 1000 variables to each have mean 0 and standard deviation 1.

Apply the following commands to properly prepare the data:

```{r}
genes = read.csv("GeneExpression.csv",header=F)
genesNew = t(genes); dim(genesNew)
genesNew = scale(genesNew)
```
```{r}
#transpose the dataframe, and scale the values
x.scale = scale(t(genes))
means1 = apply(x.scale[1:20,],2,mean)
means2 = apply(x.scale[21:40,],2,mean)
hist(c(means1))
```
```{r}
hist(c(means2))
```
You should wind up with a data frame of size 40 rows (tissue samples) by 1000 columns (gene expression measurements) - use this to complete the following tasks.
 
### Question 19 **(1 point)**k:  Clustering

Based on the goal of the study, explain why it makes sense to split the data into only two clusters.

**Text Answer**:Since the goal of the study is to distinguish between healthy and diseased tissue samples - this is a binary outcome, hence two clusters.  



### Question 20 **(1 point)**k: 20

Use hierarchical clustering with **Euclidean distance** and **complete linkage** to split the 40 samples into *two* clusters.  How many tissue samples from among samples 21–40 are in the second cluster?

```{r,echo=FALSE}
#calculate distance in p space
distance = dist(x.scale,method = "euclidean")

#cluster using complete linkage
hc.complete = hclust(distance,method = "complete")
#plot(hc.complete)

#set clusters = 2 per study parameters
nclust=2

#assign cluster class to each observation
hc.memb = cutree(hc.complete,k=nclust)
summary(as.factor(hc.memb))

```
1  2 
20 20 

**Numeric (integer) Answer (AUTOGRADED)**  

### Question 21 **(2 points)**k:

At time of diagnosis, the actual state for tissue samples 1-20 was "healthy", and tissue samples 21–40 were "diseased".What do the results of the clustering from the previous question tell us about the ability of the gene expression measurements to identify diseased tissue?

**Text Answer**:
The results of the clustering leads us to believe the gene expression measurements 
of the diseased tissue are very good in its ability to separate healthy and diseased tissue, since it evenly aggregated 20 healthy observations in a cluster and 20 diseased samples in another. 

### Question 22 **(1 point)**WRONG:  Principal Components

Use ``prcomp`` to compute the principal components for all 40 samples.  
How many *meaningful* (that is, explaining a non-zero proportion of the variability) principle components are able to be computed?

Number of principal components = All 40 X, PROBABLY 2

```{r,echo=FALSE}
#compute PCA, and check summary.
pca.gene = prcomp(x.scale, center = T, scale = T) #changed center and scale from = F
#summary(pca.gene)
```

**Numeric (integer) Answer (AUTOGRADED)**  

### Question 23 **(2 points)**: 0.116

What is the cumulative PVE explained by the first two principal components?

(respond as a proportion, out to *three* decimal places)

```{r,echo=FALSE}
summary(pca.gene)   
```
0.08108 + 0.03449 = 0.11557
**Numeric Answer (AUTOGRADED)**  

### Question 24 **(2 points)**k:

Produce a biplot of the first two principal components and upload it on the Canvas quiz, using the "Embed Image" button.

**Graph Answer**  

```{r,echo=FALSE,fig.width=6, fig.height=6}
# plotting score vectors + loadings of first two principal components
biplot(pca.gene,choices = 1:2, scale = 0) #save and upload
```
***

### Variable Importance

Next, we compute the means of the (previously-standardized) variables for only the last twenty tissue samples (samples 21-40) – we will refer to these as the **means2**.  Code for doing so is shown below:

```{r,echo=FALSE,fig.width=5, fig.height=5}
genesNew2 = genesNew[21:40,]; dim(genesNew2)
means2 = apply(genesNew2,2,mean)
```

Recall that each variable records measurements for a distinct gene expression.

### Question 25 **(2 points)**k:

A histogram of **means2** (the 1000 means computed for the second half of samples for each of the 1000 gene expressions) is displayed below.

```{r,echo=FALSE,fig.width=6, fig.height=4}
hist(means2,breaks=30, 
     main = "Computed for Tissue Samples 21-40",
     xlab = "Means of 1000 gene expressions")
```

   * **Describe** the distribution visualized in the histogram. 
   * **Discuss** how this pattern could occur, even when the gene-expressions (across the full data set) having been standardized.

**Text Answer**:   
 The distribution shows most of the gene-expression means centered around 0 (as expected, after centering the data), and the distribution of that bigger subset is approximately symmetric. But there is also a small peak of higher gene-expression means, which suggests that a subset of gene-expressions have higher means for the last 20 tissue samples as compared to the first 20 tissue samples.  

### Question 26 **(2 points)**:

A notable feature of the plot of the **means2** values is that there is a separated (higher) group of gene-expression means, as computed on only the last twenty tissue samples (samples 21–40). 

This separated set  contains  means that are (lower than, about the same as, *higher than) expected for standardized gene-expressions;
The means of the gene-expressions computed for the first twenty tissue samples (samples 1-20) would thus be (larger negative values than, *about the same values as, larger values than) expected.  

**Multiple Dropdowns Answer (AUTOGRADED)**  


### Question 27 **(2 points)**: 

We can use the below code to compute **pc.loadings1**, the loadings for principal component 1 fit to the full data: 

```{r}
genes.pc = prcomp(genesNew)
pc.loadings1 = genes.pc$rotation[,1] 
```

At the end of this question, we see a plot of **pc.loadings1** against **means2** (the means of all 1000 variables for only the last twenty tissue samples, samples 21–40). 

Use values from the prior code definition of **means2**, along with this plot, to select two variables (from the list below) that are most important in the first principal component.  You may also find the biplot to be helpful.

```{r}
#added
#select two variables (from the list below) that are most important in the first principal component
biplot(genes.pc,choices = 1:2, scale = 0)
```
```{r,echo=F,fig.width=5, fig.height=4.5}
ImportantVars = data.frame(means2,pc.loadings1)
ImportantVars %>% 
  gf_point(pc.loadings1 ~ means2,size=2,shape = 20) %>%
  gf_labs(title = "",
          y = "Loadings for principal component 1, fit to full data", x = "Means of 1000 gene expressions for Tissue Samples 21-40")
```

```{r}
##how to determine importance?
summary(genes.pc)$importance
plot(genes.pc)
# cumulative PVE by direct computation
 genes.pc$sdev
  vjs = genes.pc$sdev^2
  pve = vjs/sum(vjs); pve
  cumsum(pve)  
# cumulative PVE directly from output
CumulativePVE <- summary(genes.pc)$importance[3,]; CumulativePVE

plot(CumulativePVE, type = "o", ylab="Cumulative PVE", xlab="Principal Component")

```
#select two variables (from the list below) that are most important in the first principal component
#points close together correspond to observations with similar scores. 
```{r}
####PC Components and Visuals####
# loadings of principal components
pc.info$rotation  
pc.loadings1 = pc.info$rotation[,1]  # loadings for first principal component
pc.loadings2 = pc.info$rotation[,2]  # loadings for second principal component
pc1scores = pc.info$x[,1]  # first principal component score vector
pc2scores = pc.info$x[,2]  # second principal component score vector
```

**Multiple select (AUTOGRADED)**:  two of  
Var 95 
Var 564 <------ 
Var 568 <------ 
Var 703 
Var 907 
